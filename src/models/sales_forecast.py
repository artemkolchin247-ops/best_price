"""Sales forecasting module.

Provides two models: Log-Log linear regression and RandomForestRegressor.
Includes cross-validation and model selection. Exposes `SalesForecaster` with
`fit(df)` and `predict_sales(price, features_row)` methods.
"""
from typing import List, Optional, Dict, Any, Tuple

import numpy as np
import pandas as pd
import time
import uuid
import logging
from sklearn.linear_model import LinearRegression, PoissonRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
from sklearn.isotonic import IsotonicRegression

logger = logging.getLogger(__name__)


class BaseModel:
    def fit(self, X: pd.DataFrame, y: pd.Series):
        raise NotImplementedError

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        raise NotImplementedError

    def get_elasticity(self) -> float:
        return 0.0


class LogLogModel(BaseModel):
    """Log-log linear model: ln(y+1) ~ ln(p) + other features"""

    def __init__(self):
        self.model = LinearRegression()

    def _transform_X(self, X: pd.DataFrame) -> pd.DataFrame:
        Xt = X.copy()
        for col in Xt.columns:
            # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            if pd.api.types.is_numeric_dtype(Xt[col]):
                Xt[col] = np.log(Xt[col].astype(float).clip(lower=1.0))
        return Xt

    def fit(self, X: pd.DataFrame, y: pd.Series):
        y_t = np.log1p(y)
        Xt = self._transform_X(X)
        self.model.fit(Xt, y_t)
        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        Xt = self._transform_X(X)
        pred_ln = self.model.predict(Xt)
        return np.expm1(pred_ln)

    def get_elasticity(self, feature_names: List[str]) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ —Ü–µ–Ω–µ (—ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å)."""
        if hasattr(self.model, "coef_"):
            feature_names = feature_names or []
            try:
                # –í LogLogModel –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                if "price_after_spp" in feature_names:
                    idx = feature_names.index("price_after_spp")
                    return float(self.model.coef_[idx])
                return 0.0
            except (ValueError, IndexError):
                return 0.0
        return 0.0


class RFModel(BaseModel):
    def __init__(self, **kwargs):
        self.model = RandomForestRegressor(n_estimators=100, random_state=42, **kwargs)

    def fit(self, X: pd.DataFrame, y: pd.Series):
        self.model.fit(X, y)
        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        return self.model.predict(X)


class PoissonModel(BaseModel):
    """Poisson regression for count data."""
    def __init__(self):
        # alpha=0.0 is traditional Poisson (no penalty), 
        # but alpha > 0 adds regularization.
        self.model = PoissonRegressor(alpha=0.1, max_iter=300)

    def fit(self, X: pd.DataFrame, y: pd.Series):
        self.model.fit(X, y)
        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        return self.model.predict(X)


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¢–ó 6.2)
AD_FEATURES = ["ad_internal", "ad_bloggers", "ad_vk"]

class SalesForecaster:
    """Train and select best model for sales forecasting.

    Usage:
        sf = SalesForecaster(feature_cols=[...])
        sf.fit(df)  # df must contain 'orders' and 'price_after_spp' and feature_cols
        q = sf.predict_sales(price=1200, features_row=row_dict)
    """

    def __init__(self, feature_cols: Optional[List[str]] = None, time_col: str = "date"):
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, —É—Ç–µ—á–∫–∏ –∏ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¢–ó 2.2)
        leaks = {"orders", "revenue", "profit", "margin", "margin_unit", "conversion"}
        forbidden_features = {"price_before_spp", "spp", "cogs", "logistics", "storage"}
        raw_features = feature_cols or ["price_after_spp", "day_of_week", "ad_internal", "ad_bloggers", "ad_vk"]
        self.feature_cols = [c for c in raw_features 
                           if c.lower() not in leaks and 
                           c not in forbidden_features and
                           not c.startswith('Unnamed')]
        self.time_col = time_col
        self.models: Dict[str, BaseModel] = {
            "loglog": LogLogModel(),
            "rf": RFModel(),
            "poisson": PoissonModel()
        }
        
        # Diagnostics & Quality
        self.quality_info: Dict[str, Any] = {}
        self.elasticity_info: Dict[str, Any] = {}
        self.performance_info: Dict[str, Any] = {}
        
        self.best_model_name: Optional[str] = None
        self.stability_mode: str = "S1"
        self.monotonicity_flag: str = "monotone"
        self.protective_mode: Optional[str] = None
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ (–¢–ó 2) - –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –∞—Ç—Ä–∏–±—É—Ç
        self.pipeline_log = {
            "run_id": f"run_{int(time.time())}",
            "steps": []
        }
        
        # –õ–æ–≥–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI (legacy, derived view)
        self.pipeline_logs: List[str] = []
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è gating UI (–¢–ó)
        self.data_state: str = "OK"  # OK, EMPTY, TOO_SMALL, NO_PRICE_VARIATION, FAILED
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö (–¢–ó 1)
        self.error = {
            "code": "",
            "message": "",
            "failed_step": "",
            "exception_type": "",
            "traceback_id": None
        }

    def _reset_state(self):
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ (—É–±—Ä–∞—Ç—å –∫—ç—à –ø—Ä–æ—à–ª—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤)."""
        self.best_model_name = None
        self.stability_mode = "S1"
        self.monotonicity_flag = "monotone"
        self.protective_mode = None
        self.quality_info = {}
        self.elasticity_info = {}
        self.performance_info = {}
        self.model_result = {}
        self.pipeline_logs = []
        self.data_state = "OK"
        self.error = {
            "code": "",
            "message": "",
            "failed_step": "",
            "exception_type": "",
            "traceback_id": None
        }
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π run_id –¥–ª—è –ª–æ–≥–æ–≤
        self.pipeline_log = {
            "run_id": f"run_{int(time.time())}",
            "steps": []
        }
        # pipeline_logs –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ _add_pipeline_step

    def _add_pipeline_step(self, name: str, data: pd.DataFrame, status: str = "ok", notes: str = None):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —à–∞–≥ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ª–æ–≥ –ø–∞–π–ø–ª–∞–π–Ω–∞."""
        key_fields = ['date', 'orders', 'price_after_spp', 'price_before_spp', 'spp']
        nan_counts = {}
        for field in key_fields:
            if field in data.columns:
                nan_counts[field] = int(data[field].isna().sum())
            else:
                nan_counts[field] = 0
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ date_min/date_max (–¢–ó)
        date_min = None
        date_max = None
        if self.time_col in data.columns and len(data) > 0:
            try:
                dt = pd.to_datetime(data[self.time_col], errors="coerce")
                if dt.notna().any():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –≤–∞–ª–∏–¥–Ω–∞—è –¥–∞—Ç–∞
                    date_min = dt.min()
                    date_max = dt.max()
                    date_min = date_min.isoformat() if pd.notna(date_min) else None
                    date_max = date_max.isoformat() if pd.notna(date_max) else None
            except (ValueError, TypeError):
                # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –æ—Å—Ç–∞–≤–ª—è–µ–º None
                date_min = None
                date_max = None
        
        step_info = {
            "name": name,
            "status": status,
            "rows": len(data),
            "cols": len(data.columns),
            "nan_counts": nan_counts,
            "date_min": date_min,
            "date_max": date_max,
            "notes": notes
        }
        
        self.pipeline_log["steps"].append(step_info)
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        log_message = f"üìä {name}: rows={len(data)}, NaN counts: {nan_counts}"
        if status == "failed":
            log_message = f"üö® {name}: FAILED - {notes}"
        self.pipeline_logs.append(log_message)

    def _set_error(self, code: str, message: str, failed_step: str, exception: Exception = None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ —Å –¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –∫–æ–¥–∞–º–∏."""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∞ –æ—à–∏–±–∫–∏
        error_recommendations = {
            "E_NO_FILES": [
                "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å",
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"
            ],
            "E_MISSING_COLUMNS": [
                "–î–æ–±–∞–≤—å—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: date, orders, price_after_spp",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ"
            ],
            "E_DATE_PARSE_FAILED": [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π: YYYY-MM-DD)",
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã"
            ],
            "E_NUMERIC_CAST_FAILED": [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ price –∏ orders —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞",
                "–£–¥–∞–ª–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö"
            ],
            "E_FILTER_EXCLUDED_ALL": [
                "–†–∞—Å—à–∏—Ä—å—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –¥–∞—Ç—ã –≤ —Ñ–∞–π–ª–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –ø–µ—Ä–∏–æ–¥—É"
            ],
            "E_DROPNAS_REMOVED_ALL": [
                "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –Ω–µ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö"
            ],
            "E_PIPELINE_EXCEPTION": [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ñ–æ—Ä–º–∞—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É —Å –¥–µ—Ç–∞–ª—è–º–∏ –æ—à–∏–±–∫–∏"
            ]
        }
        
        self.data_state = "FAILED"
        self.error = {
            "code": code,
            "message": message,
            "failed_step": failed_step,
            "exception_type": type(exception).__name__ if exception else "",
            "traceback_id": str(uuid.uuid4())[:8] if exception else None,
            "recommendations": error_recommendations.get(code, ["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞"])
        }

    def _validate_input_files(self, df: pd.DataFrame) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        if df is None or df.empty:
            self._set_error("E_NO_FILES", "–í—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã", "input_validation")
            return False
        return True

    def _validate_required_columns(self, df: pd.DataFrame) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫."""
        required_cols = [self.time_col, "orders", "price_after_spp"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            self._set_error(
                "E_MISSING_COLUMNS", 
                f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}", 
                "column_validation"
            )
            return False
        return True

    def _validate_date_parsing(self, df: pd.DataFrame) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç."""
        if self.time_col in df.columns:
            nat_ratio = df[self.time_col].isna().sum() / len(df)
            if nat_ratio > 0.5:  # –ë–æ–ª–µ–µ 50% –¥–∞—Ç –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏—Å—å
                self._set_error(
                    "E_DATE_PARSE_FAILED",
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å {nat_ratio:.1%} –¥–∞—Ç",
                    "date_validation"
                )
                return False
        return True

    def _validate_numeric_cast(self, df: pd.DataFrame) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–∞–º."""
        numeric_cols = ["orders", "price_after_spp"]
        for col in numeric_cols:
            if col in df.columns:
                non_numeric_ratio = pd.to_numeric(df[col], errors='coerce').isna().sum() / len(df)
                if non_numeric_ratio > 0.5:  # –ë–æ–ª–µ–µ 50% –Ω–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ —á–∏—Å–ª–∞–º
                    self._set_error(
                        "E_NUMERIC_CAST_FAILED",
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏ {non_numeric_ratio:.1%} –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ {col} –∫ —á–∏—Å–ª–∞–º",
                        "numeric_validation"
                    )
                    return False
        return True

    def _validate_filter_results(self, df_before: pd.DataFrame, df_after: pd.DataFrame, step_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∏–ª–∞ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ."""
        if len(df_after) == 0 and len(df_before) > 0:
            if step_name == "filter_period":
                self._set_error("E_FILTER_EXCLUDED_ALL", "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–µ—Ä–∏–æ–¥—É —É–¥–∞–ª–∏–ª–∞ –≤—Å–µ —Å—Ç—Ä–æ–∫–∏", step_name)
            elif step_name == "drop_invalid_rows":
                self._set_error("E_DROPNAS_REMOVED_ALL", "–û—á–∏—Å—Ç–∫–∞ –æ—Ç NaN —É–¥–∞–ª–∏–ª–∞ –≤—Å–µ —Å—Ç—Ä–æ–∫–∏", step_name)
            else:
                self._set_error("E_PIPELINE_EXCEPTION", f"–®–∞–≥ {step_name} —É–¥–∞–ª–∏–ª –≤—Å–µ –¥–∞–Ω–Ω—ã–µ", step_name)
            return False
        return True

    def _log_data_step(self, step_name: str, data: pd.DataFrame):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ NaN –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º."""
        rows_count = len(data)
        key_fields = ['date', 'orders', 'price_after_spp']
        nan_counts = {}
        for field in key_fields:
            if field in data.columns:
                nan_counts[field] = data[field].isna().sum()
            else:
                nan_counts[field] = 'N/A'
        
        log_message = f"üìä {step_name}: rows={rows_count}, NaN counts: {nan_counts}"
        self.pipeline_logs.append(log_message)
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å
        if rows_count == 0:
            critical_message = f"üö® CRITICAL: Data became empty at step: {step_name}"
            self.pipeline_logs.append(critical_message)
            logger.warning(critical_message)

    def _prepare_xy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π."""
        df2 = df.copy()
        
        # 1. load_input - –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not self._validate_input_files(df2):
            return pd.DataFrame(), pd.Series([])
        self._add_pipeline_step("load_input", df2)
        
        # 2. normalize_columns - —É–¥–∞–ª–µ–Ω–∏–µ Unnamed –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å 'Unnamed:' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        unnamed_cols = [col for col in df2.columns if col.startswith('Unnamed:')]
        if unnamed_cols:
            df2 = df2.drop(columns=unnamed_cols)
            self._add_pipeline_step("normalize_columns", df2, notes=f"Dropped {len(unnamed_cols)} Unnamed columns")
        else:
            self._add_pipeline_step("normalize_columns", df2)
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if not self._validate_required_columns(df2):
            return pd.DataFrame(), pd.Series([])
        
        # 4. parse_dates - –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç
        if self.time_col in df2.columns:
            try:
                df2[self.time_col] = pd.to_datetime(df2[self.time_col], errors="coerce")
                self._add_pipeline_step("parse_dates", df2)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç
                if not self._validate_date_parsing(df2):
                    return pd.DataFrame(), pd.Series([])
                    
            except Exception as e:
                self._set_error("E_DATE_PARSE_FAILED", f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç: {str(e)}", "parse_dates", e)
                self._add_pipeline_step("parse_dates", df2, "failed", f"Exception: {str(e)}")
                return pd.DataFrame(), pd.Series([])
        else:
            self._add_pipeline_step("parse_dates", df2, notes="No date column found")
        
        # 5. cast_numeric - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        try:
            # –ü—Ä–∏–≤–æ–¥–∏–º orders –∫ float
            if "orders" in df2.columns:
                df2["orders"] = pd.to_numeric(df2["orders"], errors="coerce")
            
            # –ü—Ä–∏–≤–æ–¥–∏–º price_after_spp –∫ float
            if "price_after_spp" in df2.columns:
                df2["price_after_spp"] = pd.to_numeric(df2["price_after_spp"], errors="coerce")
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –¥—Ä—É–≥–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ numeric –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
            for col in df2.columns:
                if col not in [self.time_col, "orders", "price_after_spp"]:
                    if df2[col].dtype == 'object':
                        try:
                            df2[col] = pd.to_numeric(df2[col], errors="coerce")
                        except (ValueError, TypeError):
                            pass  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è
            
            self._add_pipeline_step("cast_numeric", df2)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–∞–º
            if not self._validate_numeric_cast(df2):
                return pd.DataFrame(), pd.Series([])
                
        except Exception as e:
            self._set_error("E_NUMERIC_CAST_FAILED", f"–û—à–∏–±–∫–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–∏–ø–æ–≤: {str(e)}", "cast_numeric", e)
            self._add_pipeline_step("cast_numeric", df2, "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])
        
        # 6. filter_period - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if self.time_col in df2.columns:
            try:
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                valid_dates = df2[self.time_col].notna()
                df2_before = df2.copy()
                df2 = df2[valid_dates]
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                if not self._validate_filter_results(df2_before, df2, "filter_period"):
                    return pd.DataFrame(), pd.Series([])
                    
                self._add_pipeline_step("filter_period", df2, notes=f"Filtered {len(df2_before) - len(df2)} rows with invalid dates")
            except Exception as e:
                self._set_error("E_PIPELINE_EXCEPTION", f"–û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–µ—Ä–∏–æ–¥–∞: {str(e)}", "filter_period", e)
                self._add_pipeline_step("filter_period", df2, "failed", f"Exception: {str(e)}")
                return pd.DataFrame(), pd.Series([])
        else:
            self._add_pipeline_step("filter_period", df2, notes="No date column for period filtering")
        
        # 7. drop_invalid_rows - dropna –ø–æ –∫–ª—é—á–µ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            key_cols = [self.time_col, "orders", "price_after_spp"]
            key_cols = [col for col in key_cols if col in df2.columns]
            
            if key_cols:
                df2_before = df2.copy()
                df2 = df2.dropna(subset=key_cols)
                rows_dropped = len(df2_before) - len(df2)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ dropna
                if not self._validate_filter_results(df2_before, df2, "drop_invalid_rows"):
                    return pd.DataFrame(), pd.Series([])
                    
                self._add_pipeline_step("drop_invalid_rows", df2, notes=f"Dropped {rows_dropped} rows with NaN in key columns")
            else:
                self._add_pipeline_step("drop_invalid_rows", df2, notes="No key columns for NaN check")
        except Exception as e:
            self._set_error("E_DROPNAS_REMOVED_ALL", f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {str(e)}", "drop_invalid_rows", e)
            self._add_pipeline_step("drop_invalid_rows", df2, "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])
        
        # 8. –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è demand model (–¢–ó 2.2)
        try:
            forbidden_features = {"price_before_spp", "spp", "cogs", "logistics", "storage"}
            cols_to_drop = []
            for col in df2.columns:
                if col in forbidden_features:
                    cols_to_drop.append(col)
            if cols_to_drop:
                df2 = df2.drop(columns=cols_to_drop)
                self._add_pipeline_step("remove_forbidden_features", df2, notes=f"Dropped {len(cols_to_drop)} forbidden features")
        except Exception as e:
            self._set_error("E_PIPELINE_EXCEPTION", f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}", "remove_forbidden_features", e)
            self._add_pipeline_step("remove_forbidden_features", df2, "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])
        
        # 9. –û–±—Ä–∞–±–æ—Ç–∫–∞ price –∫–æ–ª–æ–Ω–∫–∏
        try:
            if "price" in df2.columns:
                if "price_after_spp" not in df2.columns:
                    df2["price_after_spp"] = df2["price"]
                # –£–¥–∞–ª—è–µ–º 'price', —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –¥—É–±–ª–µ–π-–ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
                df2 = df2.drop(columns=["price"])
                self._add_pipeline_step("handle_price_column", df2, notes="Converted price to price_after_spp")
        except Exception as e:
            self._set_error("E_PIPELINE_EXCEPTION", f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–ª–æ–Ω–∫–∏ —Ü–µ–Ω—ã: {str(e)}", "handle_price_column", e)
            self._add_pipeline_step("handle_price_column", df2, "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])
        
        # 10. feature_engineering - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        try:
            if self.time_col in df2.columns:
                df2 = df2.sort_values(self.time_col)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0=Mon, 6=Sun)
                df2["day_of_week"] = df2[self.time_col].dt.dayofweek
                self._add_pipeline_step("feature_engineering", df2, notes="Added day_of_week feature")
            else:
                self._add_pipeline_step("feature_engineering", df2, notes="No date column for feature engineering")
        except Exception as e:
            self._set_error("E_PIPELINE_EXCEPTION", f"–û—à–∏–±–∫–∞ feature engineering: {str(e)}", "feature_engineering", e)
            self._add_pipeline_step("feature_engineering", df2, "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])
        
        # 11. final_dataset - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        try:
            # –§–∏–∫—Å–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –î–õ–Ø –ú–û–î–ï–õ–ï–ô (–±–µ–∑ orders –∏ date)
            leaks = {"orders", "revenue", "profit", "margin", "margin_unit", "conversion", self.time_col}
            forbidden_features = {"price_before_spp", "spp", "cogs", "logistics", "storage"}
            
            # Ensure 'price_after_spp' is always in feature_cols if it exists in df2
            if "price_after_spp" in df2.columns and "price_after_spp" not in self.feature_cols:
                self.feature_cols.append("price_after_spp")

            # Add any other relevant columns from df2 that are not leaks and not already in feature_cols
            for col in df2.columns:
                if (col not in leaks and 
                    col not in self.feature_cols and 
                    col not in forbidden_features and
                    not col.startswith('Unnamed')):
                    self.feature_cols.append(col)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ SKU –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ SKU (one-hot encoding)
            if "sku" in df2.columns and "sku" in self.feature_cols:
                unique_skus = df2["sku"].nunique()
                if unique_skus > 1:
                    # One-hot encoding –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ SKU
                    sku_dummies = pd.get_dummies(df2["sku"], prefix="sku")
                    df2 = pd.concat([df2, sku_dummies], axis=1)
                    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É sku
                    df2 = df2.drop(columns=["sku"])
                    # –û–±–Ω–æ–≤–ª—è–µ–º feature_cols - —É–±–∏—Ä–∞–µ–º sku, –¥–æ–±–∞–≤–ª—è–µ–º one-hot –∫–æ–ª–æ–Ω–∫–∏
                    self.feature_cols = [col for col in self.feature_cols if col != "sku"]
                    self.feature_cols.extend(sku_dummies.columns.tolist())
                    self._add_pipeline_step("sku_encoding", df2, notes=f"One-hot encoded {unique_skus} SKUs: {len(sku_dummies.columns)} features")
            
            # Filter X to only include columns that are actually present in df2
            # and are in self.feature_cols, and ARE NUMERIC
            X_cols_to_use = [col for col in self.feature_cols if col in df2.columns]
            X = df2[X_cols_to_use].copy()
            
            # Select only numeric columns for models
            X = X.select_dtypes(include=[np.number])
            self.feature_cols = X.columns.tolist()
            
            self._add_pipeline_step("final_dataset", X, notes=f"Final dataset: {len(X.columns)} numeric columns")
            
            y = df2["orders"].astype(float)
            # fill NA
            X = X.fillna(0)
            
            return X, y
            
        except Exception as e:
            self._set_error("E_PIPELINE_EXCEPTION", f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}", "final_dataset", e)
            self._add_pipeline_step("final_dataset", pd.DataFrame(), "failed", f"Exception: {str(e)}")
            return pd.DataFrame(), pd.Series([])

    def _calculate_quality_metrics(self, df: pd.DataFrame):
        """–†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ data_state."""
        try:
            if df.empty:
                self.data_state = "EMPTY"
                self._set_error("EMPTY_DATA", "–í—Ö–æ–¥–Ω–æ–π DataFrame –ø—É—Å—Ç", "quality_check")
                self._add_pipeline_step("quality_check", df, "failed", "DataFrame is empty")
                return
            
            # –ß–∏—Å–ª–æ –¥–Ω–µ–π
            n_days = len(df[self.time_col].unique()) if self.time_col in df.columns else len(df)
            
            # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ü–µ–Ω
            if "price_after_spp" in df.columns:
                n_price_unique = df["price_after_spp"].nunique()
                price_cv = df["price_after_spp"].std() / df["price_after_spp"].mean() if df["price_after_spp"].mean() > 0 else 0
            else:
                n_price_unique = 0
                price_cv = 0
            
            # –î–æ–ª—è –Ω—É–ª–µ–π
            zero_share = (df["orders"] == 0).sum() / len(df) if "orders" in df.columns else 1.0
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è
            if "orders" in df.columns and "price_after_spp" in df.columns:
                corr = df["orders"].corr(df["price_after_spp"])
            else:
                corr = 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–¢–ó)
            if n_days < 7 or n_price_unique < 3:
                self.data_state = "TOO_SMALL"
                notes = f"n_days={n_days}, n_price_unique={n_price_unique}"
                self._add_pipeline_step("quality_check", df, "failed", f"Data too small: {notes}")
            elif zero_share > 0.8 or price_cv < 0.01:
                self.data_state = "NO_PRICE_VARIATION"
                notes = f"zero_share={zero_share:.2f}, price_cv={price_cv:.4f}"
                self._add_pipeline_step("quality_check", df, "failed", f"No price variation: {notes}")
            else:
                self.data_state = "OK"
                self._add_pipeline_step("quality_check", df, "ok")
            
            self.quality_info = {
                "n_days": int(n_days),
                "n_price_unique": int(n_price_unique),
                "price_cv": float(price_cv),
                "zero_share": float(zero_share),
                "corr": float(corr),
                "data_ok": (n_days >= 30 and n_price_unique >= 6 and price_cv >= 0.03),
                "data_state": self.data_state
            }
            
        except Exception as e:
            self._set_error("QUALITY_ERROR", f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {str(e)}", "quality_check", e)
            self._add_pipeline_step("quality_check", df, "failed", f"Exception: {str(e)}")
            raise

    def _detect_ad_features(self, df: pd.DataFrame) -> List[str]:
        """–ê–≤—Ç–æ-–¥–µ—Ç–µ–∫—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (–¢–ó 6.3)."""
        available_ad_features = []
        for col in AD_FEATURES:
            if col in df.columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∞ –∏–º–µ–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
                if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                    non_null_count = df[col].notna().sum()
                    non_zero_count = (df[col] != 0).sum()
                    if non_null_count >= 30 and non_zero_count >= 10:
                        available_ad_features.append(col)
                        logger.debug("Found ad feature %s with %s observations", col, non_null_count)
                    else:
                        logger.debug("Ad feature %s has insufficient data (%s obs, %s non-zero)", col, non_null_count, non_zero_count)
                else:
                    logger.debug("Ad feature %s has non-numeric dtype: %s", col, df[col].dtype)
            else:
                logger.debug("Ad feature %s not found in dataset", col)
        
        return available_ad_features

    def _build_ad_profiles(self, df: pd.DataFrame, ad_features: List[str]) -> Dict[str, Dict[str, float]]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π —Ä–µ–∫–ª–∞–º—ã Low/Med/High (–¢–ó 6.4)."""
        profiles = {"low": {}, "med": {}, "high": {}}
        
        for col in ad_features:
            # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            clean_values = df[col].dropna().drop_duplicates()
            clean_values = clean_values[clean_values != 0]  # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏
            
            if len(clean_values) < 10:
                logger.debug("Insufficient non-zero values for %s: %s", col, len(clean_values))
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                profiles["low"][col] = 0.0
                profiles["med"][col] = 0.0
                profiles["high"][col] = 0.0
                continue
            
            # Winsorize p1-p99 –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
            p1, p99 = clean_values.quantile([0.01, 0.99])
            clean_values = clean_values.clip(p1, p99)
            
            # –ö–≤–∞–Ω—Ç–∏–ª–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–µ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)
            p25, p50, p75 = clean_values.quantile([0.25, 0.5, 0.75])
            
            profiles["low"][col] = float(p25)
            profiles["med"][col] = float(p50)
            profiles["high"][col] = float(p75)
            
            logger.debug("%s profiles - low: %.2f, med: %.2f, high: %.2f", col, p25, p50, p75)
        
        return profiles

    def _get_base_features(self, df: pd.DataFrame, method: str = "last_day") -> Dict[str, float]:
        """–§–∏–∫—Å–∞—Ü–∏—è –Ω–µ—Ü–µ–Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –±–∞–∑–æ–≤–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ (–¢–ó 6.5)."""
        X, _ = self._prepare_xy(df)
        base_features = {}
        
        if method == "last_day":
            # –í–∞—Ä–∏–∞–Ω—Ç A: –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è
            last_row = X.iloc[-1].to_dict()
        elif method == "median":
            # –í–∞—Ä–∏–∞–Ω—Ç B: –º–µ–¥–∏–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∏—Å—Ç–æ—Ä–∏–∏
            last_row = X.median().to_dict()
        elif method == "typical_day":
            # –í–∞—Ä–∏–∞–Ω—Ç C: —Ç–∏–ø–∏—á–Ω—ã–π –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ + –º–µ–¥–∏–∞–Ω—ã
            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏
            if "day_of_week" in X.columns:
                typical_day = X["day_of_week"].mode().iloc[0]
                typical_data = X[X["day_of_week"] == typical_day]
                last_row = typical_data.median().to_dict()
            else:
                last_row = X.median().to_dict()
        else:
            last_row = X.iloc[-1].to_dict()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        price_cols = ["price_after_spp", "price_before_spp", "spp", "price"]
        for col in price_cols:
            if col in last_row:
                del last_row[col]
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è float
        for col, val in last_row.items():
            try:
                base_features[col] = float(val)
            except (ValueError, TypeError):
                base_features[col] = 0.0
        
        return base_features

    def _calculate_ad_profiles(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ñ–∏–ª–µ–π —Ä–µ–∫–ª–∞–º—ã (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥)."""
        # –ê–≤—Ç–æ-–¥–µ—Ç–µ–∫—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        ad_features = self._detect_ad_features(df)
        
        if not ad_features:
            logger.debug("No valid ad features found")
            return None  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¢–ó 3.1)
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π
        profiles = self._build_ad_profiles(df, ad_features)
        profiles["method"] = "last_day"  # –ú–µ—Ç–æ–¥ —Ñ–∏–∫—Å–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        profiles["available_features"] = ad_features
        
        return profiles

    def _calculate_numerical_elasticity(self, df: pd.DataFrame, ad_profile: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —á–∏—Å–ª–µ–Ω–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ —Å–µ—Ç–∫–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏."""
        X, _ = self._prepare_xy(df)
        if "price_after_spp" not in X.columns:
            return {"elasticity_med": 0.0, "elasticity_iqr": 0.0, "mono_violations": 0.0, "grid_info": []}

        p_min, p_max = X["price_after_spp"].min(), X["price_after_spp"].max()
        if p_max <= p_min:
            return {"elasticity_med": 0.0, "elasticity_iqr": 0.0, "mono_violations": 0.0, "grid_info": []}

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å —Ä–µ–∫–ª–∞–º—ã –∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¢–ó 6.5)
        if ad_profile is None:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å med –ø—Ä–æ—Ñ–∏–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            ad_profiles = self._calculate_ad_profiles(df)
            if ad_profiles is not None and ad_profiles.get("available_features"):
                ad_profile = {k: v for k, v in ad_profiles["med"].items() if k in ad_profiles["available_features"]}
            else:
                ad_profile = {}  # –ü—É—Å—Ç–æ–π –ø—Ä–æ—Ñ–∏–ª—å –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        # –§–∏–∫—Å–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        base_features = self._get_base_features(df, method="last_day")
        
        # –°—Ç—Ä–æ–∏–º —Å–µ—Ç–∫—É (20 —Ç–æ—á–µ–∫)
        p_grid = np.linspace(p_min, p_max, 20)
        
        preds = []
        for p in p_grid:
            # –§–æ—Ä–º–∏—Ä—É–µ–º features_row —Å —Ü–µ–Ω–æ–π –∏ –ø—Ä–æ—Ñ–∏–ª–µ–º —Ä–µ–∫–ª–∞–º—ã
            features_row = {"price_after_spp": p}
            features_row.update(base_features)  # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_row.update(ad_profile) if ad_profile else None  # –†–µ–∫–ª–∞–º–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
            
            q = max(float(self.predict_sales(p, features_row=features_row)), 1e-6)
            preds.append(q)
        
        preds = np.array(preds)
        q_grid_raw = preds.copy()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        
        # –°—á–∏—Ç–∞–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ —Å—ã—Ä–æ–π –∫—Ä–∏–≤–æ–π (–¢–ó 4.2)
        violations_raw = 0
        for i in range(len(preds) - 1):
            if preds[i+1] > preds[i] * 1.03:  # threshold = 3%
                violations_raw += 1
        mono_v_raw = violations_raw / (len(preds) - 1)
        
        # –ü—Ä–∞–≤–∏–ª–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (–¢–ó 4.2)
        if mono_v_raw > 0.2:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º isotonic regression (–Ω–µ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∞—è)
            from sklearn.isotonic import IsotonicRegression
            ir = IsotonicRegression(increasing=False, out_of_bounds='clip')
            preds_calibrated = ir.fit_transform(p_grid, preds)
            q_grid_used = preds_calibrated
        else:
            preds_calibrated = preds
            q_grid_used = preds
        
        # –°—á–∏—Ç–∞–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π –∫—Ä–∏–≤–æ–π
        violations_used = 0
        for i in range(len(q_grid_used) - 1):
            if q_grid_used[i+1] > q_grid_used[i] * 1.03:  # threshold = 3%
                violations_used += 1
        mono_v_used = violations_used / (len(q_grid_used) - 1)
        
        # –°—á–∏—Ç–∞–µ–º —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ q_grid_used (–¢–ó 4.3)
        ln_p = np.log(p_grid)
        ln_q = np.log(q_grid_used)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—É—é –∫—Ä–∏–≤—É—é
        
        # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è
        valid_mask = np.isfinite(ln_p) & np.isfinite(ln_q)
        ln_p_valid = ln_p[valid_mask]
        ln_q_valid = ln_q[valid_mask]
        
        if len(ln_p_valid) < 2:
            return {"elasticity_med": 0.0, "elasticity_iqr": 0.0, "mono_violations": mono_v_used, "grid_info": []}
        
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ OLS —Ä–µ–≥—Ä–µ—Å—Å–∏—é ln(q) = a + b*ln(p) (–¢–ó 4.1)
        from sklearn.linear_model import LinearRegression
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º X = ln(p_grid), Y = ln(q_grid_used)
        ln_p_grid = np.log(p_grid)
        ln_q_grid = np.log(np.clip(q_grid_used, 1e-6, None))
        
        # –£–¥–∞–ª—è–µ–º NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è
        valid_mask = np.isfinite(ln_p_grid) & np.isfinite(ln_q_grid)
        ln_p_valid = ln_p_grid[valid_mask]
        ln_q_valid = ln_q_grid[valid_mask]
        
        n_points = len(ln_p_valid)
        if n_points < 3:
            # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            e_global = None
            r2 = None
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é Y (–¢–ó 4.2)
            y_var = np.var(ln_q_valid)
            if y_var < 1e-8:
                # –î–∏—Å–ø–µ—Ä—Å–∏—è —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞, —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞
                e_global = None
                r2 = None
            else:
                # OLS —Ä–µ–≥—Ä–µ—Å—Å–∏—è ln(q) = a + b*ln(p)
                X_reg = ln_p_valid.reshape(-1, 1)
                Y_reg = ln_q_valid
                
                reg = LinearRegression()
                reg.fit(X_reg, Y_reg)
                e_global = float(reg.coef_[0])  # –ù–∞–∫–ª–æ–Ω b = –≥–ª–æ–±–∞–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å
                r2 = float(reg.score(X_reg, Y_reg))
        
        # –†–∞—Å—á–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–Ω–æ—Å—Ç–∏ (–¢–ó 2.1)
        e_grid_local = []
        eps = 1e-6
        
        # Edge-case: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ (–¢–ó 2.3)
        if len(p_grid) < 5:
            return {
                "elasticity_med": None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º None –¥–ª—è edge-cases
                "elasticity_iqr": None,
                "beta_median": None,
                "beta_iqr": None,
                "mono_violations": mono_v_used,
                "mono_violations_raw": mono_v_raw,
                "e_grid": None,
                "q_grid": q_grid_used.tolist(),
                "q_grid_raw": q_grid_raw.tolist(),
                "q_grid_calibrated": preds_calibrated.tolist() if mono_v_raw > 0.2 else None,
                "r_squared": r2,
                "e_stats": {"min": 0, "median": 0, "max": 0, "std": 0, "len": 0},
                "global_regression": {
                    "global_elasticity": e_global,
                    "r_squared": r2,
                    "n_points": n_points
                },
                "calibrated": mono_v_raw > 0.2,
                "insufficient_data": True
            }
        
        # –ö–ª–∏–ø–ø–∏—Ä—É–µ–º q —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å log(0) –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        q_clipped = np.clip(q_grid_used, eps, None)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º q_grid_used (–¢–ó 2.2)
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ç–æ—á–µ–∫
        for i in range(1, len(p_grid) - 1):
            ln_q_i_minus_1 = np.log(q_clipped[i-1])
            ln_q_i_plus_1 = np.log(q_clipped[i+1])
            ln_p_i_minus_1 = np.log(p_grid[i-1])
            ln_p_i_plus_1 = np.log(p_grid[i+1])
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            denominator = ln_p_i_plus_1 - ln_p_i_minus_1
            if abs(denominator) > 1e-10:
                e_i = (ln_q_i_plus_1 - ln_q_i_minus_1) / denominator  # –§–æ—Ä–º—É–ª–∞ –∏–∑ –¢–ó 2.1
                e_grid_local.append(e_i)  # –ù–µ –æ–∫—Ä—É–≥–ª—è–µ–º (–¢–ó 2.2)
            else:
                e_grid_local.append(0.0)
        
        # e_grid —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ç–æ—á–∫–∏ (–∫—Ä–∞—è –Ω–µ —Å—á–∏—Ç–∞–µ–º)
        e_grid_with_nan = [np.nan] + e_grid_local + [np.nan]  # –î–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if len(e_grid_local) > 0:
            e_stats = {
                "min": float(np.min(e_grid_local)),
                "median": float(np.median(e_grid_local)),
                "max": float(np.max(e_grid_local)),
                "std": float(np.std(e_grid_local)),
                "len": len(e_grid_local),
                "valid_points": len(e_grid_local),  # –î–ª—è UI
                "total_points": len(e_grid_local)  # –î–ª—è UI
            }
        else:
            e_stats = {
                "min": 0, "median": 0, "max": 0, "std": 0, "len": 0,
                "valid_points": 0,  # –î–ª—è UI
                "total_points": 0  # –î–ª—è UI
            }
        # IQR —Å—á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ –±—É—Ç—Å—Ç—Ä–∞–ø –ø–æ —Ç–æ—á–∫–∞–º —Å–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        bootstrap_elasticities = []
        n_bootstrap = 50
        np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        
        for _ in range(n_bootstrap):
            # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
            indices = np.random.choice(len(ln_p_valid), size=len(ln_p_valid), replace=True)
            if len(np.unique(indices)) < 2:
                continue
                
            p_boot = ln_p_valid[indices]
            q_boot = ln_q_valid[indices]
            
            reg_boot = LinearRegression()
            reg_boot.fit(p_boot.reshape(-1, 1), q_boot)
            bootstrap_elasticities.append(float(reg_boot.coef_[0]))
        
        if bootstrap_elasticities:
            q75, q25 = np.percentile(bootstrap_elasticities, [75, 25])
            e_iqr = float(q75 - q25)
        else:
            e_iqr = 0.0
        
        return {
            "elasticity_med": e_global,  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∏–∑ OLS
            "elasticity_iqr": e_iqr,
            "beta_median": e_global,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å UI
            "beta_iqr": e_iqr,    # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å UI
            "mono_violations": mono_v_used,  # –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –∫—Ä–∏–≤–æ–π
            "mono_violations_raw": mono_v_raw,  # –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ —Å—ã—Ä–æ–π –∫—Ä–∏–≤–æ–π
            "e_grid": e_grid_with_nan,  # –õ–æ–∫–∞–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ —Å–µ—Ç–∫–µ
            "q_grid": q_grid_used.tolist(),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –∫—Ä–∏–≤–∞—è (–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–ª–∏ —Å—ã—Ä–∞—è)
            "q_grid_raw": q_grid_raw.tolist(),    # –°—ã—Ä–∞—è –∫—Ä–∏–≤–∞—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            "q_grid_calibrated": preds_calibrated.tolist() if mono_v_raw > 0.2 else None,  # –ö–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–∏–≤–∞—è
            "r_squared": r2,  # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            "e_stats": e_stats,  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            "global_regression": {  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
                "global_elasticity": e_global,
                "r_squared": r2,
                "n_points": n_points
            },
            "calibrated": mono_v_raw > 0.2  # –ë—ã–ª–∞ –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞
        }

    def cross_validate(self, df: pd.DataFrame, n_splits: int = 3) -> Dict[str, float]:
        X, y = self._prepare_xy(df)
        tscv = TimeSeriesSplit(n_splits=n_splits)
        scores: Dict[str, List[float]] = {name: [] for name in self.models.keys()}
        
        # –î–ª—è LogLog –∑–∞–º–µ—Ä—è–µ–º —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ —Ñ–æ–ª–¥–∞–º (–¢–ó 2.1)
        loglog_betas = []
        
        # –î–ª—è baseline calculation
        baseline_scores = []

        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # Baseline: rolling mean prediction (–¢–ó 7.1)
            if len(y_train) >= 7:
                baseline_pred = [y_train.tail(7).mean()] * len(y_val)
            else:
                baseline_pred = [y_train.mean()] * len(y_val)
            baseline_rmse = float(np.sqrt(mean_squared_error(y_val, baseline_pred)))
            baseline_scores.append(baseline_rmse)
            
            for name, model in self.models.items():
                if name == "loglog":
                    m = LogLogModel()
                elif name == "rf":
                    m = RFModel()
                else:
                    m = PoissonModel()
                
                try:
                    m.fit(X_train, y_train)
                    pred = m.predict(X_val)
                    rmse = float(np.sqrt(mean_squared_error(y_val, pred)))
                    if name == "loglog":
                        loglog_betas.append(m.get_elasticity(self.feature_cols))
                except (ValueError, RuntimeError):
                    rmse = float("inf")
                scores[name].append(rmse)

        avg_scores = {name: float(np.mean(vals)) for name, vals in scores.items()}
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º baseline_rmse –≤ performance_info (–¢–ó 7.2)
        if baseline_scores:
            self.performance_info["baseline_rmse"] = float(np.mean(baseline_scores))
        else:
            self.performance_info["baseline_rmse"] = 0.0
            
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π RMSE, –∏—Å–∫–ª—é—á–∞—è –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        valid_scores = [score for score in avg_scores.values() if score != float("inf")]
        if valid_scores:
            self.performance_info["best_rmse"] = float(min(valid_scores))
        else:
            self.performance_info["best_rmse"] = float("inf")
            
        # –†–∞—Å—á–∏—Ç—ã–≤–∞–µ–º improvement vs baseline
        if (self.performance_info["baseline_rmse"] > 0 and 
            self.performance_info["best_rmse"] != float("inf")):
            improvement = 1 - self.performance_info["best_rmse"] / self.performance_info["baseline_rmse"]
            self.performance_info["improvement_vs_baseline"] = float(improvement)
        else:
            self.performance_info["improvement_vs_baseline"] = 0.0
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ LogLog —Ñ–æ–ª–¥–∞–º
        if loglog_betas:
            self.elasticity_info["loglog_cv_med"] = float(np.median(loglog_betas))
            q75, q25 = np.percentile(loglog_betas, [75, 25])
            self.elasticity_info["loglog_cv_iqr"] = float(q75 - q25)

        return avg_scores

    def fit(self, df: pd.DataFrame, n_splits: int = 3) -> str:
        """Train models with CV and select best by RMSE. Returns best model name."""
        
        # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è (—É–±—Ä–∞—Ç—å –∫—ç—à –ø—Ä–æ—à–ª—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤)
        self._reset_state()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç fit –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –æ—à–∏–±–æ–∫
        self._fit_return_value = None
        
        try:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ–≥–¥–∞ –ø–∏—à–µ—Ç—Å—è, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –æ–±—É—á–µ–Ω–∏—è (–¢–ó)
            self._add_pipeline_step("raw_rows", df)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ SKU –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ SKU
            if "sku" in df.columns:
                unique_skus = df["sku"].nunique()
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ SKU –¥–ª—è debug
                self.elasticity_info["unique_sku"] = unique_skus
                
                if unique_skus == 1:
                    # –ï—Å–ª–∏ –æ–¥–∏–Ω SKU - —É–¥–∞–ª—è–µ–º sku –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    self.feature_cols = [col for col in self.feature_cols if col != "sku"]
                    self._add_pipeline_step("sku_processing", df, "ok", f"Single SKU detected: removed 'sku' from features")
                else:
                    # –ï—Å–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ SKU - –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è one-hot/target encoding
                    self._add_pipeline_step("sku_processing", df, "ok", f"Multiple SKUs detected: {unique_skus} SKUs, keeping 'sku' for encoding")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ SKU, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –æ–¥–∏–Ω SKU
                self.elasticity_info["unique_sku"] = 1
            
            # –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            self._calculate_quality_metrics(df)
            
            # Gating: –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            if self.data_state in ["TOO_SMALL", "NO_PRICE_VARIATION", "EMPTY"]:
                self._add_pipeline_step("training_stopped", pd.DataFrame(), "failed", f"Data state: {self.data_state}")
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º best_model_name –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è "Model not trained"
                self.best_model_name = "NO_MODEL"  # –Ø–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è gating
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç fit
                self._fit_return_value = "NO_MODEL"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º model_result –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º (–¢–ó)
                self.model_result = {
                    "elasticity": {},
                    "quality": self.quality_info,
                    "stability_mode": "S2",         # –∏–ª–∏ –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º, –Ω–æ –Ω–µ UNKNOWN
                    "monotonicity_flag": "N/A",
                    "improvement_vs_baseline": 0.0,
                    "protective_mode": "scenario",
                    "performance": {},
                    "model_name": None,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç
                    "features_used": self.feature_cols,
                    "data_state": self.data_state,
                    "error": self.error,
                    "pipeline_log": self.pipeline_log
                }
                
                return "NO_MODEL"
            
            self._add_pipeline_step("data_quality_ok", df)
            scores = self.cross_validate(df, n_splits=n_splits)
            self.best_model_name = min(scores.items(), key=lambda x: x[1])[0]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±–æ—Ä–µ –º–æ–¥–µ–ª–∏
            best_score = scores[self.best_model_name]
            model_comparison = {name: score for name, score in scores.items()}
            self.performance_info["model_selection"] = {
                "chosen_model": self.best_model_name,
                "chosen_score": best_score,
                "all_scores": model_comparison,
                "selection_reason": self._get_model_selection_reason(scores, self.best_model_name)
            }

            # 2. –û–ë–£–ß–ê–ï–ú –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            X, y = self._prepare_xy(df)
            
            # 3. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
            combined_data = X.copy()
            combined_data['orders'] = y
            self._add_pipeline_step("final_rows_for_model_rows", combined_data)
            
            if self.best_model_name == "loglog":
                self.models["loglog"].fit(X, y)
            elif self.best_model_name == "rf":
                self.models["rf"].fit(X, y)
            else:
                self.models["poisson"].fit(X, y)

            # 2. –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ñ–∏–ª–µ–π —Ä–µ–∫–ª–∞–º—ã –∏ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è (–¢–ó 6.6)
            ad_profiles = self._calculate_ad_profiles(df)
            self.elasticity_info["ad_profiles"] = ad_profiles
            
            # –†–∞—Å—á–µ—Ç —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è —Ä–µ–∫–ª–∞–º—ã (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)
            if ad_profiles is not None:
                profile_names = ["low", "med", "high"]
                for profile_name in profile_names:
                    if profile_name in ad_profiles and ad_profiles.get("available_features"):
                        profile_data = {k: v for k, v in ad_profiles[profile_name].items() 
                                      if k in ad_profiles["available_features"]}
                        num_e = self._calculate_numerical_elasticity(df, ad_profile=profile_data)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –ø—Ä–æ—Ñ–∏–ª—è
                        for key, value in num_e.items():
                            if key not in ["ad_profiles"]:  # –ò–∑–±–µ–≥–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–∏
                                self.elasticity_info[f"{profile_name}_{key}"] = value
                    else:
                        # –ï—Å–ª–∏ –ø—Ä–æ—Ñ–∏–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∑–∞–ø–æ–ª–Ω—è–µ–º None
                        for key in ["elasticity_med", "elasticity_iqr", "mono_violations"]:
                            self.elasticity_info[f"{profile_name}_{key}"] = None
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å - –ø–æ med –ø—Ä–æ—Ñ–∏–ª—é
            if "med" in ad_profiles and ad_profiles.get("available_features"):
                med_profile = {k: v for k, v in ad_profiles["med"].items() 
                              if k in ad_profiles["available_features"]}
                med_e = self._calculate_numerical_elasticity(df, ad_profile=med_profile)
            else:
                med_e = self._calculate_numerical_elasticity(df)
            
            self.elasticity_info.update(med_e)
            
            # 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ (–¢–ó 5)
            iqr = med_e.get("elasticity_iqr", 0)
            if iqr is None:
                iqr = 0.0  # Default –µ—Å–ª–∏ None
                
            if iqr <= 0.3:
                self.stability_mode = "S1"
            elif iqr <= 0.7:
                self.stability_mode = "S2"
            else:
                self.stability_mode = "S3"
                
            # –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å (–∏—Å–ø–æ–ª—å–∑—É–µ–º mono_violations –∏–∑ q_grid_used)
            if med_e["mono_violations"] > 0.2:
                self.monotonicity_flag = "non_monotone"
            else:
                self.monotonicity_flag = "monotone"

            # 4. Protective Mode - —É—Ç–æ—á–Ω–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ (–¢–ó 7.2)
            improvement = self.performance_info.get("improvement_vs_baseline", 0)
            data_ok = self.quality_info.get("data_ok", False)
            mono_v = med_e["mono_violations"]
            stability = self.stability_mode
            mono_flag = "non_monotone" if mono_v > 0.2 else "monotone"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self.elasticity_info["protective_logic"] = {
                "improvement": improvement,
                "data_ok": data_ok,
                "stability_mode": stability,
                "monotonicity_flag": mono_flag,
                "mono_violations": mono_v
            }
            
            # –ü—Ä–∞–≤–∏–ª–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Å –ø–æ—Ä–æ–≥–∞–º–∏ –¥–æ–≤–µ—Ä–∏—è (–æ–±–Ω–æ–≤–ª–µ–Ω–æ)
            if not data_ok or improvement < 0.05:
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –ø–ª–æ—Ö–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ improvement < 5%
                self.protective_mode = "scenario"
                reason = f"scenario (improvement={improvement:.3f} < 0.05 or data_ok={data_ok})"
            elif improvement < 0.10:
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: improvement 5-10% ‚Üí conservative
                self.protective_mode = "conservative"
                reason = f"conservative (improvement={improvement:.3f} in 0.05-0.10 range)"
            elif stability == "S3" or mono_flag == "non_monotone":
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: S3 –∏–ª–∏ –Ω–µ–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ improvement >= 10%)
                self.protective_mode = "conservative"
                reason = f"conservative (stability={stability} or monotonicity={mono_flag}, improvement={improvement:.3f} >= 0.10)"
            elif stability == "S1" and mono_flag == "monotone":
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4: —Ö–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è + improvement >= 10%
                self.protective_mode = None
                reason = f"normal (S1 + monotone + improvement={improvement:.3f} >= 0.10)"
            else:
                # S2 + –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å - –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ improvement >= 10%)
                self.protective_mode = "conservative"
                reason = f"conservative (S2 + monotone, improvement={improvement:.3f} >= 0.10)"
            
            self.elasticity_info["protective_logic"]["reason"] = reason

            # Sanity Check (–¢–ó 4.1)
            corr = self.quality_info.get("corr", 0)
            if corr < -0.2 and abs(med_e["elasticity_med"]) < 0.05:
                self.quality_info["sanity_warning"] = True

            # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–¢–ó 1.1)
            self.model_result = {
                "elasticity": self.elasticity_info,
                "quality": self.quality_info,
                "stability_mode": self.stability_mode,
                "monotonicity_flag": self.monotonicity_flag,
                "improvement_vs_baseline": self.performance_info.get("improvement_vs_baseline", 0),
                "protective_mode": self.protective_mode,
                "performance": self.performance_info,
                "model_name": self.best_model_name,
                "features_used": self.feature_cols,
                "data_state": self.data_state,  # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è gating UI
                "error": self.error,  # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
                "pipeline_log": self.pipeline_log,  # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏
                "unique_sku": self.elasticity_info.get("unique_sku", 1)  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ SKU
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç fit
            self._fit_return_value = self.best_model_name

            return self.best_model_name
            
        except Exception as e:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –ª–æ–≥–∞ –æ–± –æ—à–∏–±–∫–µ
            self._set_error("FIT_ERROR", f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}", "model_training", e)
            self._add_pipeline_step("model_training", pd.DataFrame(), "failed", f"Exception: {str(e)}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
            self.model_result = {
                "elasticity": {},
                "quality": self.quality_info,
                "stability_mode": "S2",  # –ù–µ UNKNOWN, –∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "monotonicity_flag": "monotone",
                "improvement_vs_baseline": 0,
                "protective_mode": "scenario",
                "performance": {},
                "model_name": None,
                "features_used": [],
                "data_state": self.data_state,
                "error": self.error,
                "pipeline_log": self.pipeline_log,
                "unique_sku": self.elasticity_info.get("unique_sku", 1)  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ SKU
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç fit —Å –æ—à–∏–±–∫–æ–π
            self._fit_return_value = "FAILED"
            
            return "FAILED"  # –ù–µ raise, –∞ return –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è UX/–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

    def calibrate_curve(self, prices: np.ndarray, preds: np.ndarray) -> np.ndarray:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ –∫—Ä–∏–≤–æ–π —Å–ø—Ä–æ—Å–∞."""
        if len(prices) < 2:
            return preds
        
        # The policy_info and allow_pos logic is removed as per the new fit method.
        # The calibration logic should be updated based on the new stability/monotonicity flags if needed.
        # For now, keeping the original logic but noting the change in context.
        
        # If monotonicity_flag is "non_monotone" and stability_mode is not "S1" (data is good enough)
        # then we might want to enforce monotonicity.
        # For simplicity, let's assume if we need to calibrate, we enforce decreasing.
        
        # –ñ—ë—Å—Ç–∫–∞—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å: —Å–ø—Ä–æ—Å –Ω–µ –¥–æ–ª–∂–µ–Ω —Ä–∞—Å—Ç–∏ –ø—Ä–∏ —Ä–æ—Å—Ç–µ —Ü–µ–Ω—ã
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º cummin (–Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞) –∏–ª–∏ IsotonicRegression
        ir = IsotonicRegression(increasing=False, out_of_bounds='clip')
        preds_calibrated = ir.fit_transform(prices, preds)
        return preds_calibrated

    def _get_model_selection_reason(self, scores: Dict[str, float], chosen_model: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–Ω—è—Ç–Ω—É—é –ø—Ä–∏—á–∏–Ω—É –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏."""
        if not scores:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ RMSE (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        sorted_models = sorted(scores.items(), key=lambda x: x[1])
        chosen_score = scores[chosen_model]
        
        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ –¥—Ä—É–≥–∏—Ö
        if len(sorted_models) >= 2:
            second_best = sorted_models[1]
            score_diff = second_best[1] - chosen_score
            
            if score_diff > 0.1:  # –ó–Ω–∞—á–∏–º–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
                return f"–ù–∞–∏–º–µ–Ω—å—à–∞—è –æ—à–∏–±–∫–∞ (RMSE={chosen_score:.3f}) - –ª—É—á—à–µ {second_best[0]} –Ω–∞ {score_diff:.3f}"
            elif score_diff > 0.01:  # –ù–µ–±–æ–ª—å—à–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
                return f"–ù–µ–º–Ω–æ–≥–æ –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (RMSE={chosen_score:.3f}) - –ª—É—á—à–µ {second_best[0]} –Ω–∞ {score_diff:.3f}"
            else:  # –û—á–µ–Ω—å –±–ª–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                return f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (RMSE={chosen_score:.3f}) - –º–æ–¥–µ–ª–∏ –æ—á–µ–Ω—å –±–ª–∏–∑–∫–∏"
        
        return f"–ù–∞–∏–º–µ–Ω—å—à–∞—è –æ—à–∏–±–∫–∞ (RMSE={chosen_score:.3f})"

    def get_model_result(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –µ–¥–∏–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥–µ–ª–∏ (–¢–ó 1.1)."""
        if not hasattr(self, 'model_result'):
            return {}
        result = self.model_result.copy()
        result["pipeline_log"] = self.pipeline_log.copy()  # –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –∞—Ç—Ä–∏–±—É—Ç
        # pipeline_logs –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        return result

    def get_pipeline_logs(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (legacy view)."""
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º derived view –∏–∑ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ pipeline_log
        logs = []
        for step in self.pipeline_log.get("steps", []):
            status_emoji = "‚úÖ" if step["status"] == "ok" else "‚ùå"
            log_message = f"{status_emoji} {step['name']}: rows={step['rows']}, cols={step['cols']}"
            if step.get("notes"):
                log_message += f" - {step['notes']}"
            logs.append(log_message)
        return logs

    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–µ."""
        if self.best_model_name is None:
            return {}
        info = {
            "model_name": self.best_model_name,
            "quality": self.quality_info,
            "elasticity": self.elasticity_info,
            "stability_mode": self.stability_mode,
            "monotonicity_flag": self.monotonicity_flag,
            "protective_mode": self.protective_mode,
            "performance": self.performance_info
        }
        if self.best_model_name == "loglog":
            info["actual_elasticity"] = self.models["loglog"].get_elasticity(self.feature_cols)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
        if "model_selection" in self.performance_info:
            info["model_selection_reason"] = self.performance_info["model_selection"]["selection_reason"]
        
        return info

    def predict_sales(self, price: float, features_row: Optional[Dict[str, Any]] = None) -> float:
        """Predict sales quantity for a given price and optional other features."""
        # Debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–æ–π –ø—Ä–æ–≥–Ω–æ–∑–∞ (–¢–ó)
        logger.debug("model_name: %s", self.best_model_name)
        logger.debug("data_state: %s", getattr(self, "data_state", "UNKNOWN"))
        logger.debug("fit_return: %s", getattr(self, "_fit_return_value", "UNKNOWN"))
        
        if self.best_model_name is None:
            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –º–æ–¥–µ–ª–∏
            context_parts = []
            
            # best_model_name
            context_parts.append(f"best_model_name={self.best_model_name}")
            
            # data_state
            data_state = getattr(self, 'data_state', 'UNKNOWN')
            context_parts.append(f"data_state={data_state}")
            
            # error –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            error = getattr(self, 'error', {})
            if error.get('code'):
                context_parts.append(f"error.code={error['code']}")
            if error.get('message'):
                context_parts.append(f"error.message={error['message']}")
            if error.get('failed_step'):
                context_parts.append(f"failed_step={error['failed_step']}")
            
            # fit_return_value (–µ—Å–ª–∏ –µ—Å—Ç—å)
            fit_return = getattr(self, '_fit_return_value', None)
            if fit_return:
                context_parts.append(f"fit_return={fit_return}")
            
            context = ", ".join(context_parts)
            raise RuntimeError(f"Model not trained: {context}")
        
        if self.best_model_name == "NO_MODEL":
            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑-–∑–∞ –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            data_state = getattr(self, 'data_state', 'UNKNOWN')
            raise RuntimeError(f"–ü—Ä–æ–≥–Ω–æ–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö - {data_state}. –£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.")

        # build feature vector
        row = {c: 0 for c in self.feature_cols}
        # set price to the primary feature column
        found_price = False
        for price_col in ["price_after_spp", "price_before_spp"]:
            if price_col in self.feature_cols:
                row[price_col] = price
        if features_row:
            for k, v in features_row.items():
                if k in row:
                    row[k] = v
        
        # 2. –¢–æ–ª—å–∫–æ –ü–û–¢–û–ú –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        # df_feats = df.copy() # This line is not needed here, as we are building a single row
        # If 'price' is provided in features_row, ensure it's mapped to 'price_after_spp'
        if features_row and "price" in features_row and "price_after_spp" not in row:
            row["price_after_spp"] = features_row["price"]
        
        # day_of_week
        if "day_of_week" in self.feature_cols and row.get("day_of_week") is None:
            import datetime
            row["day_of_week"] = datetime.datetime.now().weekday()
        
        # set default ad spend to 0 if not provided
        for ad_col in ["ad_internal", "ad_bloggers", "ad_vk"]:
            if ad_col in self.feature_cols and row.get(ad_col) is None:
                row[ad_col] = 0.0

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –∏—Ö –ø–æ—Ä—è–¥–æ–∫
        for col in self.feature_cols:
            if col not in row:
                row[col] = 0.0
        
        Xnew = pd.DataFrame([row])[self.feature_cols]
        model = self.models[self.best_model_name]
        # –í–∞–∂–Ω–æ: predict —É –º–æ–¥–µ–ª—å–Ω–æ–≥–æ –æ–±–µ—Ä—Ç–∫–∏ (–Ω–∞–ø—Ä. LogLogModel) 
        # –¥–æ–ª–∂–µ–Ω —Å–∞–º –≤—ã–∑—ã–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é.
        pred = model.predict(Xnew)[0]
        return float(max(0.0, pred))

    def predict_on_df(self, df: pd.DataFrame) -> pd.Series:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)."""
        if self.best_model_name is None:
            return pd.Series([0.0] * len(df))
        
        if self.best_model_name == "NO_MODEL":
            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑-–∑–∞ –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            return pd.Series([0.0] * len(df))
        
        X, _ = self._prepare_xy(df)
        model = self.models[self.best_model_name]
        return pd.Series(model.predict(X), index=df.index)


__all__ = ["SalesForecaster", "LogLogModel", "RFModel", "PoissonModel"]
